name: Automated Video Scraping

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      website_url:
        description: 'Website URL to scrape (optional, uses links.txt if empty)'
        required: false
        type: string
      max_pages:
        description: 'Maximum pages to crawl'
        required: false
        default: '10'
        type: string

jobs:
  scrape-videos:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hour timeout
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install-deps
          playwright install chromium
      
      - name: Setup environment variables
        run: |
          echo "BUNNY_API_KEY=${{ secrets.BUNNY_API_KEY }}" >> $GITHUB_ENV
          echo "BUNNY_LIBRARY_ID=${{ secrets.BUNNY_LIBRARY_ID }}" >> $GITHUB_ENV
          echo "MAX_WORKERS=2" >> $GITHUB_ENV
      
      - name: Run video scraper
        run: |
          cd video_engine
          python main.py
        continue-on-error: true
      
      - name: Upload database artifact
        uses: actions/upload-artifact@v4
        with:
          name: video-database-${{ github.run_number }}
          path: video_engine/video_tracker.db
          retention-days: 30
      
      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: video_engine/pipeline.log
          retention-days: 7
      
      - name: Upload to Hugging Face Dataset (Optional)
        if: success()
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          pip install huggingface_hub
          python -c "
          from huggingface_hub import HfApi
          import os
          
          if os.getenv('HF_TOKEN'):
              api = HfApi()
              try:
                  api.upload_file(
                      path_or_fileobj='video_engine/video_tracker.db',
                      path_in_repo='video_tracker.db',
                      repo_id='${{ github.repository_owner }}/video-scraper-data',
                      repo_type='dataset',
                      token=os.getenv('HF_TOKEN')
                  )
                  print('âœ… Database uploaded to Hugging Face')
              except Exception as e:
                  print(f'âš ï¸ HF upload skipped: {e}')
          else:
              print('âš ï¸ HF_TOKEN not set, skipping upload')
          "
        continue-on-error: true
      
      - name: Generate summary
        if: always()
        run: |
          echo "## ðŸ“Š Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f video_engine/video_tracker.db ]; then
            pip install -q sqlite3 2>/dev/null || true
            python -c "
          import sqlite3
          import os
          
          if os.path.exists('video_engine/video_tracker.db'):
              conn = sqlite3.connect('video_engine/video_tracker.db')
              cursor = conn.cursor()
              cursor.execute('SELECT status, COUNT(*) FROM videos GROUP BY status')
              stats = cursor.fetchall()
              conn.close()
              
              print('| Status | Count |')
              print('|--------|-------|')
              for status, count in stats:
                  print(f'| {status} | {count} |')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Database not found" >> $GITHUB_STEP_SUMMARY
          fi
